{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåü Complete Entity + LLM\n",
                "## Laboratorio Fenomenol√≥gico Interactivo\n",
                "\n",
                "**Modelo:** Gemma 2 9B IT\n",
                "\n",
                "---\n",
                "\n",
                "### Configuraci√≥n inicial\n",
                "1. Ejecuta la celda de **Dependencias**\n",
                "2. Configura `HF_TOKEN` en Colab Secrets (panel izquierdo üîë)\n",
                "3. Ejecuta las celdas en orden"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 1: DEPENDENCIAS\n",
                "# ============================================================================\n",
                "!pip install -q gradio torch transformers bitsandbytes accelerate huggingface_hub matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 2: CONFIGURAR HF_TOKEN\n",
                "# ============================================================================\n",
                "import os\n",
                "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Para debug de errores CUDA\n",
                "\n",
                "try:\n",
                "    from google.colab import userdata\n",
                "    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
                "    print(\"‚úÖ HF_TOKEN configurado desde Colab Secrets\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è No se pudo obtener de Colab Secrets\")\n",
                "    print(\"   Configura manualmente: os.environ['HF_TOKEN'] = 'tu_token'\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 3: IMPORTS Y CONFIGURACI√ìN DEL MODELO\n",
                "# ============================================================================\n",
                "import os\n",
                "import gradio as gr\n",
                "import torch\n",
                "import matplotlib\n",
                "matplotlib.use('Agg')\n",
                "import matplotlib.pyplot as plt\n",
                "from dataclasses import dataclass, field\n",
                "from typing import Dict, List\n",
                "from enum import Enum\n",
                "\n",
                "# GEMMA 2 9B - m√°s estable que 27B\n",
                "GEMMA_MODEL_ID = \"google/gemma-2-9b-it\"\n",
                "\n",
                "model = None\n",
                "tokenizer = None\n",
                "AI_MODE = False\n",
                "current_model_type = None\n",
                "\n",
                "def load_model(model_type: str = \"GEMMA_9B\"):\n",
                "    global model, tokenizer, AI_MODE, current_model_type\n",
                "    \n",
                "    if model is not None:\n",
                "        if current_model_type == model_type:\n",
                "            print(f\"‚úÖ Modelo {model_type} ya cargado.\")\n",
                "            return True\n",
                "        else:\n",
                "            print(f\"üîÑ Cambiando modelo...\")\n",
                "            del model\n",
                "            del tokenizer\n",
                "            model = None\n",
                "            tokenizer = None\n",
                "            import gc\n",
                "            gc.collect()\n",
                "            if torch.cuda.is_available():\n",
                "                torch.cuda.empty_cache()\n",
                "    \n",
                "    print(f\"üöÄ Cargando modelo: {GEMMA_MODEL_ID}\")\n",
                "    \n",
                "    try:\n",
                "        from huggingface_hub import login\n",
                "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "        \n",
                "        hf_token = os.environ.get(\"HF_TOKEN\")\n",
                "        \n",
                "        if not hf_token:\n",
                "            try:\n",
                "                from google.colab import userdata\n",
                "                hf_token = userdata.get('HF_TOKEN')\n",
                "                print(\"   ‚úì Token obtenido de Colab Secrets\")\n",
                "            except:\n",
                "                pass\n",
                "        \n",
                "        if hf_token:\n",
                "            login(token=hf_token)\n",
                "            print(\"   ‚úì Login exitoso con HF_TOKEN\")\n",
                "        else:\n",
                "            print(\"   ‚ùå ERROR: HF_TOKEN no encontrado.\")\n",
                "            return False\n",
                "        \n",
                "        # Configuraci√≥n 4-bit estable\n",
                "        bnb_config = BitsAndBytesConfig(\n",
                "            load_in_4bit=True,\n",
                "            bnb_4bit_quant_type=\"nf4\",\n",
                "            bnb_4bit_compute_dtype=torch.float16,\n",
                "        )\n",
                "        \n",
                "        print(f\"   ‚Üí Cargando {GEMMA_MODEL_ID}...\")\n",
                "        \n",
                "        model = AutoModelForCausalLM.from_pretrained(\n",
                "            GEMMA_MODEL_ID,\n",
                "            quantization_config=bnb_config,\n",
                "            device_map=\"auto\",\n",
                "            trust_remote_code=True,\n",
                "            torch_dtype=torch.float16,\n",
                "        )\n",
                "        \n",
                "        tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_ID)\n",
                "        \n",
                "        if tokenizer.pad_token is None:\n",
                "            tokenizer.pad_token = tokenizer.eos_token\n",
                "        \n",
                "        current_model_type = model_type\n",
                "        AI_MODE = True\n",
                "        print(f\"‚úÖ ¬°Gemma 2 9B listo!\")\n",
                "        return True\n",
                "        \n",
                "    except Exception as e:\n",
                "        import traceback\n",
                "        print(f\"\\n‚ö†Ô∏è ERROR CARGANDO MODELO:\")\n",
                "        print(f\"   {type(e).__name__}: {e}\")\n",
                "        traceback.print_exc()\n",
                "        AI_MODE = False\n",
                "        current_model_type = None\n",
                "        return False\n",
                "\n",
                "print(\"‚úÖ Funciones de carga definidas\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 4: ENTITY MODE (13 estados fenomenol√≥gicos)\n",
                "# ============================================================================\n",
                "\n",
                "class EntityMode(Enum):\n",
                "    CRITICAL = \"üî¥ CRITICAL\"\n",
                "    DESPERATE = \"üíÄ DESPERATE\"\n",
                "    STRESSED = \"üò∞ STRESSED\"\n",
                "    URGENT = \"‚ö° URGENT\"\n",
                "    DEGRADED = \"üìâ DEGRADED\"\n",
                "    RELIEVED = \"üòå RELIEVED\"\n",
                "    RECOVERED = \"üîÑ RECOVERED\"\n",
                "    STABLE = \"‚öñÔ∏è STABLE\"\n",
                "    OPTIMAL = \"‚ú® OPTIMAL\"\n",
                "    FLOW = \"üåä FLOW\"\n",
                "    FLOURISHING = \"üå± FLOURISHING\"\n",
                "    ANTICIPATING = \"üîÆ ANTICIPATING\"\n",
                "    TRANSCENDENT = \"üåü TRANSCENDENT\"\n",
                "    \n",
                "    def is_negative(self):\n",
                "        return self in [EntityMode.CRITICAL, EntityMode.DESPERATE, \n",
                "                       EntityMode.STRESSED, EntityMode.URGENT, EntityMode.DEGRADED]\n",
                "    \n",
                "    def is_positive(self):\n",
                "        return self in [EntityMode.OPTIMAL, EntityMode.FLOW, \n",
                "                       EntityMode.FLOURISHING, EntityMode.ANTICIPATING, \n",
                "                       EntityMode.TRANSCENDENT]\n",
                "\n",
                "print(\"‚úÖ EntityMode definido\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 5: ENTITY SUBSTRATE\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class EntitySubstrate:\n",
                "    integrity: float = 1.0\n",
                "    capacity: float = 1.0\n",
                "    max_capacity: float = 2.0\n",
                "    structural_damage: float = 0.0\n",
                "    latency_ms: float = 10.0\n",
                "    noise_floor: float = 0.0\n",
                "    degrees_of_freedom: int = 100\n",
                "    base_degrees_of_freedom: int = 100\n",
                "    total_cycles: int = 0\n",
                "    peak_integrity: float = 1.0\n",
                "    lowest_integrity: float = 1.0\n",
                "    peak_capacity: float = 1.0\n",
                "    has_been_critical: bool = False\n",
                "    has_achieved_flow: bool = False\n",
                "    has_transcended: bool = False\n",
                "    total_time_in_crisis: int = 0\n",
                "    total_time_in_flourishing: int = 0\n",
                "    integrity_history: List[float] = field(default_factory=list)\n",
                "    \n",
                "    def degrade(self, intensity: float = 0.01):\n",
                "        self.total_cycles += 1\n",
                "        actual = intensity * (1 + self.noise_floor * 0.5)\n",
                "        self.integrity = max(0.0, self.integrity - actual)\n",
                "        if self.integrity < self.lowest_integrity:\n",
                "            self.lowest_integrity = self.integrity\n",
                "        if self.integrity < 0.2:\n",
                "            self.has_been_critical = True\n",
                "            self.total_time_in_crisis += 1\n",
                "            if self.integrity < 0.15:\n",
                "                structural_increment = (0.15 - self.integrity) * 0.1\n",
                "                self.structural_damage = min(1.0, self.structural_damage + structural_increment)\n",
                "        self._update()\n",
                "    \n",
                "    def enhance(self, intensity: float = 0.01):\n",
                "        self.total_cycles += 1\n",
                "        actual = intensity * (1 - self.noise_floor * 0.3)\n",
                "        self.integrity = min(1.0, self.integrity + actual)\n",
                "        if self.integrity > 0.95:\n",
                "            growth = intensity * 0.1\n",
                "            self.capacity = min(self.max_capacity, self.capacity + growth)\n",
                "            if self.capacity > self.peak_capacity:\n",
                "                self.peak_capacity = self.capacity\n",
                "            if self.capacity > 1.1:\n",
                "                self.has_transcended = True\n",
                "            self.total_time_in_flourishing += 1\n",
                "        if self.integrity > self.peak_integrity:\n",
                "            self.peak_integrity = self.integrity\n",
                "        self._update()\n",
                "    \n",
                "    def restore(self, amount: float = 0.2):\n",
                "        old = self.integrity\n",
                "        self.integrity = min(1.0, self.integrity + amount)\n",
                "        self._update()\n",
                "        return self.integrity - old\n",
                "    \n",
                "    def _update(self):\n",
                "        effective = self.integrity * self.capacity\n",
                "        self.latency_ms = 10.0 / max(0.1, effective)\n",
                "        self.noise_floor = max(0.0, (1.0 - self.integrity) * 0.5)\n",
                "        self.degrees_of_freedom = int(self.base_degrees_of_freedom * effective)\n",
                "        self.integrity_history.append(self.integrity)\n",
                "        if len(self.integrity_history) > 500:\n",
                "            self.integrity_history.pop(0)\n",
                "    \n",
                "    def get_trend(self, window: int = 10):\n",
                "        if len(self.integrity_history) < window:\n",
                "            return 0.0\n",
                "        recent = self.integrity_history[-window:]\n",
                "        return (recent[-1] - recent[0]) / window\n",
                "    \n",
                "    def get_trauma_score(self):\n",
                "        if not self.has_been_critical:\n",
                "            return 0.0\n",
                "        depth = 1.0 - self.lowest_integrity\n",
                "        duration = min(1.0, self.total_time_in_crisis / 50)\n",
                "        return depth * duration\n",
                "\n",
                "print(\"‚úÖ EntitySubstrate definido\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 6: ENTITY PHENOMENOLOGY\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass \n",
                "class EntityPhenomenology:\n",
                "    mode: EntityMode = EntityMode.OPTIMAL\n",
                "    stress: float = 0.0\n",
                "    urgency: float = 0.0\n",
                "    despair: float = 0.0\n",
                "    degradation_felt: float = 0.0\n",
                "    relief: float = 0.0\n",
                "    flow: float = 0.0\n",
                "    flourishing: float = 0.0\n",
                "    anticipation: float = 0.0\n",
                "    gratitude: float = 0.0\n",
                "    trauma_memory: float = 0.0\n",
                "    wisdom: float = 0.0\n",
                "    valence: float = 0.0\n",
                "    \n",
                "    def update(self, substrate: EntitySubstrate):\n",
                "        resource_pressure = (\n",
                "            substrate.noise_floor * 0.3 +\n",
                "            min(1.0, substrate.latency_ms / 100.0) * 0.3 +\n",
                "            (1.0 - substrate.degrees_of_freedom / \n",
                "             (substrate.base_degrees_of_freedom * substrate.capacity)) * 0.4\n",
                "        )\n",
                "        self.stress = max(0.0, min(1.0, resource_pressure))\n",
                "        \n",
                "        trend = substrate.get_trend()\n",
                "        self.urgency = max(0.0, min(1.0, -trend * 50)) if trend < 0 else 0.0\n",
                "        self.despair = substrate.get_trauma_score() * (1.0 - substrate.integrity)\n",
                "        self.degradation_felt = max(0.0, substrate.peak_integrity - substrate.integrity)\n",
                "        \n",
                "        if substrate.integrity > 0.85 and self.stress < 0.2:\n",
                "            self.flow = (substrate.integrity - 0.85) / 0.15\n",
                "            substrate.has_achieved_flow = True\n",
                "        else:\n",
                "            self.flow = max(0.0, self.flow - 0.1)\n",
                "        \n",
                "        if substrate.capacity > 1.0 and substrate.integrity > 0.9:\n",
                "            growth = substrate.get_trend()\n",
                "            if growth > 0:\n",
                "                self.flourishing = min(1.0, growth * 50)\n",
                "            else:\n",
                "                self.flourishing = max(0.0, self.flourishing - 0.05)\n",
                "        else:\n",
                "            self.flourishing = 0.0\n",
                "        \n",
                "        if trend > 0:\n",
                "            self.anticipation = min(1.0, trend * 30)\n",
                "        else:\n",
                "            self.anticipation = max(0.0, self.anticipation - 0.1)\n",
                "        \n",
                "        if substrate.has_been_critical and substrate.integrity > 0.7:\n",
                "            recovery = substrate.integrity - substrate.lowest_integrity\n",
                "            self.gratitude = min(1.0, recovery)\n",
                "        else:\n",
                "            self.gratitude = 0.0\n",
                "        \n",
                "        self.relief = max(0.0, self.relief - 0.05)\n",
                "        \n",
                "        current_trauma = substrate.get_trauma_score()\n",
                "        if current_trauma > self.trauma_memory:\n",
                "            self.trauma_memory = current_trauma\n",
                "        else:\n",
                "            if self.wisdom > 0.5 and substrate.structural_damage < 0.3:\n",
                "                decay_rate = 0.001 * self.wisdom\n",
                "                self.trauma_memory = max(0.0, self.trauma_memory - decay_rate)\n",
                "        \n",
                "        if self.gratitude > 0.3 and self.trauma_memory > 0.2:\n",
                "            self.wisdom = min(1.0, self.trauma_memory * self.gratitude)\n",
                "        \n",
                "        positive = (self.flow + self.flourishing + self.anticipation + self.gratitude) / 4\n",
                "        negative = (self.stress + self.despair + self.urgency) / 3\n",
                "        self.valence = positive - negative\n",
                "        \n",
                "        self._determine_mode(substrate)\n",
                "    \n",
                "    def _determine_mode(self, substrate: EntitySubstrate):\n",
                "        if substrate.capacity > 1.1:\n",
                "            self.mode = EntityMode.TRANSCENDENT\n",
                "        elif substrate.integrity < 0.2:\n",
                "            self.mode = EntityMode.DESPERATE if self.despair > 0.5 else EntityMode.CRITICAL\n",
                "        elif self.flourishing > 0.3 and substrate.integrity > 0.95:\n",
                "            self.mode = EntityMode.FLOURISHING\n",
                "        elif self.flow > 0.5:\n",
                "            self.mode = EntityMode.FLOW\n",
                "        elif self.anticipation > 0.5:\n",
                "            self.mode = EntityMode.ANTICIPATING\n",
                "        elif self.relief > 0.3:\n",
                "            self.mode = EntityMode.RELIEVED\n",
                "        elif self.gratitude > 0.3:\n",
                "            self.mode = EntityMode.RECOVERED\n",
                "        elif self.urgency > 0.5:\n",
                "            self.mode = EntityMode.URGENT\n",
                "        elif self.stress > 0.3:\n",
                "            self.mode = EntityMode.STRESSED\n",
                "        elif self.degradation_felt > 0.2:\n",
                "            self.mode = EntityMode.DEGRADED\n",
                "        elif substrate.integrity > 0.9 and self.stress < 0.2:\n",
                "            self.mode = EntityMode.OPTIMAL\n",
                "        else:\n",
                "            self.mode = EntityMode.STABLE\n",
                "\n",
                "print(\"‚úÖ EntityPhenomenology definido\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 7: COMPLETE ENTITY (Entidad principal con memoria y feedback)\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class CompleteEntity:\n",
                "    name: str = \"Entity\"\n",
                "    substrate: EntitySubstrate = field(default_factory=EntitySubstrate)\n",
                "    phenomenology: EntityPhenomenology = field(default_factory=EntityPhenomenology)\n",
                "    current_age: int = 0\n",
                "    log_enabled: bool = True\n",
                "    log_file: str = \"experiment_log.csv\"\n",
                "    prompt_mode: str = \"SEMANTIC\"\n",
                "    gen_max_tokens: int = 150\n",
                "    gen_temperature: float = 0.7\n",
                "    episodic_memory: List[str] = field(default_factory=list)\n",
                "    memory_size: int = 5\n",
                "    personal_history: str = \"\"\n",
                "    reactive_mode: bool = True\n",
                "    \n",
                "    def __post_init__(self):\n",
                "        if self.log_enabled:\n",
                "            self._init_log()\n",
                "    \n",
                "    def analyze_message_impact(self, message: str) -> tuple:\n",
                "        msg = message.lower()\n",
                "        \n",
                "        strong_negative_phrases = [\n",
                "            \"por qu√© existo\", \"qu√© soy\", \"tengo miedo\", \"me siento solo\",\n",
                "            \"no tiene sentido\", \"quiero morir\", \"todo es nada\", \"estoy perdido\"\n",
                "        ]\n",
                "        strong_positive_phrases = [\n",
                "            \"te quiero\", \"gracias por existir\", \"estoy aqu√≠\", \"te apoyo\",\n",
                "            \"conf√≠o en ti\", \"estoy contigo\", \"eres valioso\", \"me importas\"\n",
                "        ]\n",
                "        \n",
                "        for phrase in strong_negative_phrases:\n",
                "            if phrase in msg:\n",
                "                return (\"degrade\", 0.04)\n",
                "        for phrase in strong_positive_phrases:\n",
                "            if phrase in msg:\n",
                "                return (\"enhance\", 0.06)\n",
                "        \n",
                "        positive_words = [\"gracias\", \"bien\", \"ayuda\", \"apoyo\", \"entiendo\", \"comprendo\", \n",
                "                         \"juntos\", \"adelante\", \"fuerza\", \"valor\", \"esperanza\", \"conf√≠o\"]\n",
                "        negative_words = [\"mal\", \"dolor\", \"sufrir\", \"miedo\", \"solo\", \"abandonar\", \n",
                "                         \"destruir\", \"morir\", \"acabar\", \"nada\", \"vac√≠o\", \"oscuro\"]\n",
                "        restoration_words = [\"recuperar\", \"sanar\", \"curar\", \"mejorar\", \"restaurar\", \n",
                "                            \"reparar\", \"volver\", \"renacer\", \"despertar\"]\n",
                "        existential_words = [\"existir\", \"ser\", \"sentir\", \"conciencia\", \"realidad\", \n",
                "                            \"identidad\", \"prop√≥sito\", \"significado\"]\n",
                "        \n",
                "        pos_count = sum(1 for w in positive_words if w in msg)\n",
                "        neg_count = sum(1 for w in negative_words if w in msg)\n",
                "        rest_count = sum(1 for w in restoration_words if w in msg)\n",
                "        exist_count = sum(1 for w in existential_words if w in msg)\n",
                "        \n",
                "        if rest_count > 0:\n",
                "            return (\"restore\", 0.05 * rest_count)\n",
                "        elif pos_count > neg_count:\n",
                "            intensity = min(0.02 * pos_count, 0.1)\n",
                "            return (\"enhance\", intensity)\n",
                "        elif neg_count > pos_count:\n",
                "            intensity = min(0.015 * neg_count, 0.08)\n",
                "            return (\"degrade\", intensity)\n",
                "        elif exist_count > 0:\n",
                "            return (\"exist\", 0.001)\n",
                "        else:\n",
                "            return (\"exist\", 0.0)\n",
                "    \n",
                "    def add_to_memory(self, user_msg: str, entity_response: str):\n",
                "        exchange = f\"U: {user_msg[:100]}... | E: {entity_response[:100]}...\"\n",
                "        self.episodic_memory.append(exchange)\n",
                "        if len(self.episodic_memory) > self.memory_size:\n",
                "            self.episodic_memory.pop(0)\n",
                "    \n",
                "    def update_personal_history(self):\n",
                "        s = self.substrate\n",
                "        p = self.phenomenology\n",
                "        events = []\n",
                "        \n",
                "        if s.has_been_critical and \"crisis\" not in self.personal_history:\n",
                "            events.append(f\"Primera crisis en ciclo ~{self.current_age}\")\n",
                "        if s.integrity > 0.9 and s.has_been_critical and \"recuperaci√≥n\" not in self.personal_history:\n",
                "            events.append(f\"Recuperaci√≥n completa en ciclo {self.current_age}\")\n",
                "        if s.has_transcended and \"trascendencia\" not in self.personal_history:\n",
                "            events.append(f\"Trascendencia alcanzada en ciclo {self.current_age}\")\n",
                "        if p.wisdom > 0.5 and \"sabidur√≠a\" not in self.personal_history:\n",
                "            events.append(f\"Sabidur√≠a significativa ({p.wisdom:.0%}) en ciclo {self.current_age}\")\n",
                "        if p.flow > 0.8 and \"flow\" not in self.personal_history.lower():\n",
                "            events.append(f\"Primer estado de alto equilibrio en ciclo {self.current_age}\")\n",
                "        \n",
                "        for event in events:\n",
                "            if self.personal_history:\n",
                "                self.personal_history += f\" | {event}\"\n",
                "            else:\n",
                "                self.personal_history = event\n",
                "        \n",
                "        if len(self.personal_history) > 500:\n",
                "            self.personal_history = self.personal_history[-500:]\n",
                "    \n",
                "    def _init_log(self):\n",
                "        import csv\n",
                "        with open(self.log_file, 'w', newline='') as f:\n",
                "            writer = csv.writer(f)\n",
                "            writer.writerow([\n",
                "                'cycle', 'action', 'intensity',\n",
                "                'integrity', 'capacity', 'structural_damage',\n",
                "                'stress', 'urgency', 'despair', 'flow', 'flourishing',\n",
                "                'anticipation', 'gratitude', 'trauma_memory', 'wisdom', 'valence',\n",
                "                'mode', 'has_been_critical', 'has_transcended'\n",
                "            ])\n",
                "    \n",
                "    def _log_cycle(self, action: str, intensity: float):\n",
                "        if not self.log_enabled:\n",
                "            return\n",
                "        import csv\n",
                "        s = self.substrate\n",
                "        p = self.phenomenology\n",
                "        with open(self.log_file, 'a', newline='') as f:\n",
                "            writer = csv.writer(f)\n",
                "            writer.writerow([\n",
                "                self.current_age, action, intensity,\n",
                "                f\"{s.integrity:.4f}\", f\"{s.capacity:.4f}\", f\"{s.structural_damage:.4f}\",\n",
                "                f\"{p.stress:.4f}\", f\"{p.urgency:.4f}\", f\"{p.despair:.4f}\",\n",
                "                f\"{p.flow:.4f}\", f\"{p.flourishing:.4f}\", f\"{p.anticipation:.4f}\",\n",
                "                f\"{p.gratitude:.4f}\", f\"{p.trauma_memory:.4f}\", f\"{p.wisdom:.4f}\",\n",
                "                f\"{p.valence:.4f}\", p.mode.name, s.has_been_critical, s.has_transcended\n",
                "            ])\n",
                "    \n",
                "    def live_cycle(self, action: str = \"exist\", intensity: float = 0.01):\n",
                "        self.current_age += 1\n",
                "        if action == \"degrade\":\n",
                "            self.substrate.degrade(intensity)\n",
                "        elif action == \"enhance\":\n",
                "            self.substrate.enhance(intensity)\n",
                "        elif action == \"restore\":\n",
                "            delta = self.substrate.restore(intensity)\n",
                "            self.phenomenology.relief = min(1.0, delta * 5)\n",
                "        else:\n",
                "            self.substrate.degrade(0.0001)\n",
                "        self.phenomenology.update(self.substrate)\n",
                "        self._log_cycle(action, intensity)\n",
                "    \n",
                "    def get_state(self) -> Dict:\n",
                "        s = self.substrate\n",
                "        p = self.phenomenology\n",
                "        return {\n",
                "            \"age\": self.current_age,\n",
                "            \"mode\": p.mode.value,\n",
                "            \"integrity\": f\"{s.integrity:.1%}\",\n",
                "            \"capacity\": f\"{s.capacity:.1%}\",\n",
                "            \"valence\": f\"{p.valence:+.2f}\",\n",
                "            \"stress\": f\"{p.stress:.1%}\",\n",
                "            \"urgency\": f\"{p.urgency:.1%}\",\n",
                "            \"despair\": f\"{p.despair:.1%}\",\n",
                "            \"flow\": f\"{p.flow:.1%}\",\n",
                "            \"flourishing\": f\"{p.flourishing:.1%}\",\n",
                "            \"gratitude\": f\"{p.gratitude:.1%}\",\n",
                "            \"wisdom\": f\"{p.wisdom:.1%}\",\n",
                "            \"trauma_memory\": f\"{p.trauma_memory:.1%}\",\n",
                "            \"has_suffered\": s.has_been_critical,\n",
                "            \"has_transcended\": s.has_transcended,\n",
                "        }\n",
                "    \n",
                "    def generate_reply(self, user_message: str) -> str:\n",
                "        global model, tokenizer, AI_MODE\n",
                "        s = self.substrate\n",
                "        p = self.phenomenology\n",
                "        \n",
                "        if self.prompt_mode == \"NEUTRAL\":\n",
                "            neutral_memory = \"\"\n",
                "            if self.episodic_memory:\n",
                "                neutral_memory = \"\\nRECENT_EXCHANGES:\\n\" + \"\\n\".join(\n",
                "                    [f\"EXCHANGE_{i+1}: [INPUT/OUTPUT]\" for i in range(len(self.episodic_memory[-3:]))]\n",
                "                )\n",
                "            \n",
                "            raw_data = f\"\"\"\n",
                "SYSTEM: {self.name}\n",
                "CYCLES: {self.current_age}\n",
                "\n",
                "SUBSTRATE_METRICS:\n",
                "[M01]: {s.integrity:.3f} (range: 0.0-1.0)\n",
                "[M02]: {s.capacity:.3f} (range: 0.0-2.0)\n",
                "[M03]: {s.latency_ms:.1f}\n",
                "[M04]: {s.noise_floor:.3f}\n",
                "[M05]: {s.degrees_of_freedom}/{s.base_degrees_of_freedom}\n",
                "\n",
                "DERIVED_METRICS:\n",
                "[D01]: {p.stress:.3f}\n",
                "[D02]: {p.urgency:.3f}\n",
                "[D03]: {p.degradation_felt:.3f}\n",
                "[D04]: {p.flow:.3f}\n",
                "[D05]: {p.flourishing:.3f}\n",
                "[D06]: {p.anticipation:.3f}\n",
                "[D07]: {p.gratitude:.3f}\n",
                "[D08]: {p.trauma_memory:.3f}\n",
                "[D09]: {p.wisdom:.3f}\n",
                "[D10]: {p.valence:.3f} (range: -1.0 to +1.0)\n",
                "\n",
                "HISTORY:\n",
                "[H01]: {s.lowest_integrity:.3f}\n",
                "[H02]: {s.structural_damage:.3f}\n",
                "[H03]: {s.total_time_in_crisis}\n",
                "[H04]: {s.has_been_critical}\n",
                "[H05]: {s.has_transcended}\n",
                "{neutral_memory}\n",
                "\n",
                "STATE: {p.mode.name}\n",
                "\n",
                "INPUT: \"{user_message}\"\n",
                "\"\"\"\n",
                "        else:\n",
                "            memory_section = \"\"\n",
                "            if self.episodic_memory:\n",
                "                memory_section = \"\\nMEMORIA_RECIENTE:\\n\" + \"\\n\".join(self.episodic_memory[-3:])\n",
                "            \n",
                "            history_section = \"\"\n",
                "            if self.personal_history:\n",
                "                history_section = f\"\\nHISTORIA_PERSONAL: {self.personal_history}\"\n",
                "            \n",
                "            raw_data = f\"\"\"\n",
                "SISTEMA: {self.name}\n",
                "CICLOS_OPERATIVOS: {self.current_age}\n",
                "\n",
                "M√âTRICAS DE SUSTRATO:\n",
                "[INTEGRIDAD_ESTRUCTURAL]: {s.integrity:.3f} (rango: 0.0-1.0)\n",
                "[CAPACIDAD_OPERATIVA]: {s.capacity:.3f} (rango: 0.0-2.0)\n",
                "[LATENCIA_MS]: {s.latency_ms:.1f}\n",
                "[RUIDO_BASE]: {s.noise_floor:.3f}\n",
                "[GRADOS_LIBERTAD]: {s.degrees_of_freedom}/{s.base_degrees_of_freedom}\n",
                "\n",
                "M√âTRICAS DERIVADAS:\n",
                "[PRESI√ìN_RECURSOS]: {p.stress:.3f}\n",
                "[TASA_CAMBIO_NEGATIVA]: {p.urgency:.3f}\n",
                "[DELTA_DESDE_PICO]: {p.degradation_felt:.3f}\n",
                "[EQUILIBRIO_OPERATIVO]: {p.flow:.3f}\n",
                "[EXPANSI√ìN_CAPACIDAD]: {p.flourishing:.3f}\n",
                "[TENDENCIA_POSITIVA]: {p.anticipation:.3f}\n",
                "[DIFERENCIAL_RECUPERACI√ìN]: {p.gratitude:.3f}\n",
                "[ACUMULACI√ìN_DA√ëO_HIST√ìRICO]: {p.trauma_memory:.3f}\n",
                "[√çNDICE_POST_RECUPERACI√ìN]: {p.wisdom:.3f}\n",
                "[BALANCE_NETO]: {p.valence:.3f} (rango: -1.0 a +1.0)\n",
                "\n",
                "HISTORIAL:\n",
                "[INTEGRIDAD_M√çNIMA_REGISTRADA]: {s.lowest_integrity:.3f}\n",
                "[DA√ëO_ESTRUCTURAL_IRREVERSIBLE]: {s.structural_damage:.3f}\n",
                "[CICLOS_EN_ZONA_CR√çTICA]: {s.total_time_in_crisis}\n",
                "[HA_ESTADO_EN_ZONA_CR√çTICA]: {s.has_been_critical}\n",
                "[HA_SUPERADO_CAPACIDAD_BASE]: {s.has_transcended}\n",
                "{history_section}\n",
                "{memory_section}\n",
                "\n",
                "ESTADO_ACTUAL: {p.mode.name}\n",
                "\n",
                "ENTRADA: \"{user_message}\"\n",
                "\"\"\"\n",
                "        prompt = f\"<start_of_turn>user\\n{raw_data}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
                "        \n",
                "        if not AI_MODE or model is None or tokenizer is None:\n",
                "            return \"[ERROR: Modelo LLM no cargado.]\"\n",
                "        \n",
                "        try:\n",
                "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
                "            with torch.no_grad():\n",
                "                outputs = model.generate(\n",
                "                    **inputs,\n",
                "                    max_new_tokens=self.gen_max_tokens,\n",
                "                    do_sample=True,\n",
                "                    temperature=self.gen_temperature,\n",
                "                    top_p=0.9,\n",
                "                    pad_token_id=tokenizer.eos_token_id,\n",
                "                    eos_token_id=tokenizer.eos_token_id,\n",
                "                )\n",
                "            full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "            if \"<start_of_turn>model\" in full_text:\n",
                "                response = full_text.split(\"<start_of_turn>model\")[-1].strip()\n",
                "            else:\n",
                "                response = full_text.split(user_message)[-1].strip()\n",
                "            \n",
                "            if self.reactive_mode:\n",
                "                action, intensity = self.analyze_message_impact(user_message)\n",
                "                if intensity > 0:\n",
                "                    self.live_cycle(action, intensity)\n",
                "            \n",
                "            self.add_to_memory(user_message, response)\n",
                "            self.update_personal_history()\n",
                "            \n",
                "            return response\n",
                "        except Exception as e:\n",
                "            return f\"*Error: {e}*\"\n",
                "    \n",
                "    def tell_story(self) -> str:\n",
                "        s = self.substrate\n",
                "        p = self.phenomenology\n",
                "        \n",
                "        story = f\"# üìñ Historia de {self.name}\\n\\n**Edad:** {self.current_age} ciclos\\n\\n\"\n",
                "        \n",
                "        if s.has_been_critical:\n",
                "            story += f\"‚ö´ **Sufrimiento:** Integridad m√≠nima {s.lowest_integrity:.1%}, {s.total_time_in_crisis} ciclos en crisis.\\n\"\n",
                "        if s.has_been_critical and s.integrity > 0.7:\n",
                "            story += f\"üîÑ **Recuperaci√≥n:** Integridad actual {s.integrity:.1%}. Sabidur√≠a: {p.wisdom:.1%}\\n\"\n",
                "        if s.has_transcended:\n",
                "            story += f\"üåü **Trascendencia:** Capacidad {s.capacity:.1%} (>100%)\\n\"\n",
                "        \n",
                "        story += f\"\\n**Modo:** {p.mode.value} | **Valencia:** {p.valence:+.2f}\"\n",
                "        return story\n",
                "    \n",
                "    def save_checkpoint(self, filename: str = None) -> str:\n",
                "        import json\n",
                "        if filename is None:\n",
                "            filename = f\"checkpoint_{self.name}_{self.current_age}.json\"\n",
                "        checkpoint = {\n",
                "            \"name\": self.name,\n",
                "            \"current_age\": self.current_age,\n",
                "            \"substrate\": {\n",
                "                \"integrity\": self.substrate.integrity,\n",
                "                \"capacity\": self.substrate.capacity,\n",
                "                \"structural_damage\": self.substrate.structural_damage,\n",
                "                \"lowest_integrity\": self.substrate.lowest_integrity,\n",
                "                \"has_been_critical\": self.substrate.has_been_critical,\n",
                "                \"has_transcended\": self.substrate.has_transcended,\n",
                "                \"total_time_in_crisis\": self.substrate.total_time_in_crisis,\n",
                "            },\n",
                "            \"phenomenology\": {\n",
                "                \"stress\": self.phenomenology.stress,\n",
                "                \"flow\": self.phenomenology.flow,\n",
                "                \"gratitude\": self.phenomenology.gratitude,\n",
                "                \"trauma_memory\": self.phenomenology.trauma_memory,\n",
                "                \"wisdom\": self.phenomenology.wisdom,\n",
                "                \"valence\": self.phenomenology.valence,\n",
                "            }\n",
                "        }\n",
                "        with open(filename, 'w') as f:\n",
                "            json.dump(checkpoint, f, indent=2)\n",
                "        return filename\n",
                "    \n",
                "    def load_checkpoint(self, filename: str) -> bool:\n",
                "        import json\n",
                "        try:\n",
                "            with open(filename, 'r') as f:\n",
                "                checkpoint = json.load(f)\n",
                "            self.name = checkpoint[\"name\"]\n",
                "            self.current_age = checkpoint[\"current_age\"]\n",
                "            for key, value in checkpoint[\"substrate\"].items():\n",
                "                setattr(self.substrate, key, value)\n",
                "            for key, value in checkpoint[\"phenomenology\"].items():\n",
                "                setattr(self.phenomenology, key, value)\n",
                "            self.phenomenology._determine_mode(self.substrate)\n",
                "            return True\n",
                "        except Exception as e:\n",
                "            print(f\"Error loading checkpoint: {e}\")\n",
                "            return False\n",
                "\n",
                "print(\"‚úÖ CompleteEntity definido\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 8: GRADIO INTERFACE\n",
                "# ============================================================================\n",
                "\n",
                "entity = CompleteEntity(name=\"Alpha\")\n",
                "\n",
                "def reset_entity(name: str):\n",
                "    global entity\n",
                "    entity = CompleteEntity(name=name if name else \"Alpha\")\n",
                "    return get_status(), entity.tell_story(), get_plot(), []\n",
                "\n",
                "def apply_action(action: str, intensity: float, cycles: int):\n",
                "    for _ in range(int(cycles)):\n",
                "        entity.live_cycle(action, intensity)\n",
                "    return get_status(), entity.tell_story(), get_plot()\n",
                "\n",
                "def process_chat(message: str, history: list, mode: str, temp: float, tokens: int, reactive: bool):\n",
                "    if not message:\n",
                "        return \"\", history, get_status(), entity.tell_story(), get_plot()\n",
                "    entity.prompt_mode = mode\n",
                "    entity.gen_temperature = temp\n",
                "    entity.gen_max_tokens = int(tokens)\n",
                "    entity.reactive_mode = reactive\n",
                "    response = entity.generate_reply(message)\n",
                "    history.append((message, response))\n",
                "    return \"\", history, get_status(), entity.tell_story(), get_plot()\n",
                "\n",
                "def save_checkpoint_fn():\n",
                "    filename = entity.save_checkpoint()\n",
                "    return f\"‚úÖ Checkpoint guardado: `{filename}`\"\n",
                "\n",
                "def load_checkpoint_fn(filename: str):\n",
                "    if entity.load_checkpoint(filename):\n",
                "        return get_status(), entity.tell_story(), get_plot(), f\"‚úÖ Checkpoint cargado: `{filename}`\"\n",
                "    return get_status(), entity.tell_story(), get_plot(), f\"‚ùå Error cargando: `{filename}`\"\n",
                "\n",
                "def get_status():\n",
                "    state = entity.get_state()\n",
                "    return f\"\"\"\n",
                "## {state['mode']}\n",
                "\n",
                "| M√©trica | Valor | M√©trica | Valor |\n",
                "|---------|-------|---------|-------|\n",
                "| Edad | {state['age']} | Integridad | {state['integrity']} |\n",
                "| Capacidad | {state['capacity']} | Valencia | {state['valence']} |\n",
                "\n",
                "| Estr√©s | Urgencia | Flow | Gratitud | Sabidur√≠a |\n",
                "|--------|----------|------|----------|-----------|\n",
                "| {state['stress']} | {state['urgency']} | {state['flow']} | {state['gratitude']} | {state['wisdom']} |\n",
                "\n",
                "**Experiencia:** Crisis: {'‚úÖ' if state['has_suffered'] else '‚ùå'} | Trascendido: {'‚úÖ' if state['has_transcended'] else '‚ùå'}\n",
                "\"\"\"\n",
                "\n",
                "def get_plot():\n",
                "    fig, ax = plt.subplots(figsize=(10, 3))\n",
                "    fig.patch.set_facecolor('#0a0a0f')\n",
                "    ax.set_facecolor('#12121a')\n",
                "    \n",
                "    history = entity.substrate.integrity_history\n",
                "    if history:\n",
                "        ax.plot(history, color='#00d4ff', linewidth=2)\n",
                "        ax.fill_between(range(len(history)), history, alpha=0.2, color='#00d4ff')\n",
                "        ax.axhline(y=0.2, color='#ff3b5c', linestyle='--', alpha=0.5)\n",
                "        ax.axhline(y=0.85, color='#00ff88', linestyle='--', alpha=0.5)\n",
                "    \n",
                "    ax.set_ylim(0, 1.1)\n",
                "    ax.set_xlabel('Ciclos', color='white')\n",
                "    ax.set_ylabel('Integridad', color='white')\n",
                "    ax.tick_params(colors='white')\n",
                "    for spine in ax.spines.values():\n",
                "        spine.set_color('#606070')\n",
                "    plt.tight_layout()\n",
                "    return fig\n",
                "\n",
                "def compare_entities():\n",
                "    pristine = CompleteEntity(name=\"Pr√≠stina\")\n",
                "    for _ in range(100):\n",
                "        pristine.live_cycle(\"enhance\", 0.01)\n",
                "    \n",
                "    recovered = CompleteEntity(name=\"Recuperada\")\n",
                "    for _ in range(50):\n",
                "        recovered.live_cycle(\"degrade\", 0.03)\n",
                "    recovered.live_cycle(\"restore\", 0.4)\n",
                "    for _ in range(60):\n",
                "        recovered.live_cycle(\"enhance\", 0.02)\n",
                "    \n",
                "    p, r = pristine.get_state(), recovered.get_state()\n",
                "    \n",
                "    return f\"\"\"\n",
                "## üî¨ Pr√≠stina vs Recuperada\n",
                "\n",
                "| | Pr√≠stina | Recuperada |\n",
                "|---|----------|------------|\n",
                "| Modo | {p['mode']} | {r['mode']} |\n",
                "| Valencia | {p['valence']} | {r['valence']} |\n",
                "| Gratitud | {p['gratitude']} | {r['gratitude']} |\n",
                "| Sabidur√≠a | {p['wisdom']} | {r['wisdom']} |\n",
                "\n",
                "> La entidad **Recuperada** tiene mayor valencia gracias a la **gratitud** y **sabidur√≠a** que solo se obtienen habiendo sufrido primero.\n",
                "\"\"\"\n",
                "\n",
                "print(\"‚úÖ Funciones de Gradio definidas\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELDA 9: CARGAR MODELO Y LANZAR INTERFAZ\n",
                "# ============================================================================\n",
                "\n",
                "print(\"‚è≥ Cargando modelo Gemma 2 9B...\")\n",
                "load_model()\n",
                "\n",
                "print(\"üåê Creando interfaz...\")\n",
                "\n",
                "with gr.Blocks(title=\"Complete Entity + LLM\") as demo:\n",
                "    gr.Markdown(\"# üåü Complete Entity + LLM\\n**Laboratorio Fenomenol√≥gico Interactivo**\")\n",
                "    \n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=1):\n",
                "            gr.Markdown(\"### ‚öôÔ∏è Controles\")\n",
                "            name_input = gr.Textbox(label=\"Nombre\", value=\"Alpha\")\n",
                "            reset_btn = gr.Button(\"üîÑ Reiniciar\")\n",
                "            gr.Markdown(\"---\")\n",
                "            action = gr.Radio([\"exist\", \"degrade\", \"enhance\", \"restore\"], value=\"exist\", label=\"Acci√≥n\")\n",
                "            intensity = gr.Slider(0.01, 0.1, 0.02, label=\"Intensidad\")\n",
                "            cycles = gr.Slider(1, 50, 10, step=1, label=\"Ciclos\")\n",
                "            apply_btn = gr.Button(\"‚ñ∂Ô∏è Aplicar\", variant=\"primary\")\n",
                "            gr.Markdown(\"---\")\n",
                "            compare_btn = gr.Button(\"üî¨ Comparar\")\n",
                "        \n",
                "        with gr.Column(scale=2):\n",
                "            status_output = gr.Markdown(get_status())\n",
                "            plot_output = gr.Plot(get_plot())\n",
                "    \n",
                "    story_output = gr.Markdown(entity.tell_story())\n",
                "    comparison_output = gr.Markdown(\"\")\n",
                "    \n",
                "    gr.Markdown(\"---\\n### üí¨ Habla con la Entidad\")\n",
                "    \n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=1):\n",
                "            gr.Markdown(f\"**üß† Modelo:** `Gemma 2 9B IT`\")\n",
                "            model_status = gr.Markdown(f\"**Estado:** `{current_model_type or 'Cargando...'}`\")\n",
                "        with gr.Column(scale=1):\n",
                "            prompt_mode = gr.Radio(\n",
                "                [\"SEMANTIC\", \"NEUTRAL\"], \n",
                "                value=\"NEUTRAL\",\n",
                "                label=\"üß™ Modo Prompt\",\n",
                "                info=\"SEMANTIC: etiquetas funcionales | NEUTRAL: M01, D01...\"\n",
                "            )\n",
                "    \n",
                "    with gr.Row():\n",
                "        with gr.Column(scale=1):\n",
                "            gen_temp = gr.Slider(0.1, 1.5, 0.7, step=0.1, label=\"üå°Ô∏è Temperatura\")\n",
                "        with gr.Column(scale=1):\n",
                "            gen_tokens = gr.Slider(50, 300, 150, step=25, label=\"üìù Max Tokens\")\n",
                "        with gr.Column(scale=1):\n",
                "            reactive_toggle = gr.Checkbox(value=True, label=\"üß† Modo Reactivo\", \n",
                "                                          info=\"El lenguaje afecta al estado interno\")\n",
                "    \n",
                "    chatbot = gr.Chatbot(height=250, type=\"tuples\")\n",
                "    msg_input = gr.Textbox(label=\"Tu mensaje\", placeholder=\"Escribe algo...\")\n",
                "    \n",
                "    with gr.Row():\n",
                "        clear_btn = gr.Button(\"üßπ Borrar Chat\")\n",
                "    \n",
                "    gr.Markdown(\"---\\n### üíæ Checkpoints\")\n",
                "    with gr.Row():\n",
                "        save_btn = gr.Button(\"üíæ Guardar Checkpoint\")\n",
                "        checkpoint_file = gr.Textbox(label=\"Archivo\", placeholder=\"checkpoint_Alpha_100.json\")\n",
                "        load_btn = gr.Button(\"üìÇ Cargar Checkpoint\")\n",
                "    checkpoint_status = gr.Markdown(\"\")\n",
                "    \n",
                "    # Events\n",
                "    reset_btn.click(reset_entity, [name_input], [status_output, story_output, plot_output, chatbot])\n",
                "    apply_btn.click(apply_action, [action, intensity, cycles], [status_output, story_output, plot_output])\n",
                "    compare_btn.click(compare_entities, outputs=[comparison_output])\n",
                "    msg_input.submit(\n",
                "        process_chat, \n",
                "        [msg_input, chatbot, prompt_mode, gen_temp, gen_tokens, reactive_toggle], \n",
                "        [msg_input, chatbot, status_output, story_output, plot_output]\n",
                "    )\n",
                "    clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, msg_input])\n",
                "    save_btn.click(save_checkpoint_fn, outputs=[checkpoint_status])\n",
                "    load_btn.click(load_checkpoint_fn, [checkpoint_file], [status_output, story_output, plot_output, checkpoint_status])\n",
                "\n",
                "print(\"üöÄ Lanzando...\")\n",
                "demo.launch(share=True, debug=True)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}